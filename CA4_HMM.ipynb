{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "    <font style=\"font-family:'Courier New';font-weight:bold;color:Black\" size=\"5\">\n",
    "پروژه CA4 : استفاده از روش HMM برای POS Tagging\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import xlrd \n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "در بخش اول پس از خواندن فایل viterbi_train.xlsx، متدی به نام sents_tags_tokenize  نوشته شده که اطلاعات دادگان آموزشی را خوانده و لیستی شامل زوج مرتبهای (word,tag) بر می‌گرداند. همچنین در این متد دو تگ _START_ و _END_  به ابتدا و انتهای هر جمله در لیست خروجی اضافه شده است</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wb = xlrd.open_workbook('data/viterbi_train.xlsx') \n",
    "train_sheet = train_wb.sheet_by_index(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sents_tags_tokenize(data_sheet, padded, pad_start_symbol, pad_end_symbol):\n",
    "    word_tags = []    \n",
    "    curr_s = ''\n",
    "    for i in range(data_sheet.nrows):\n",
    "        row = data_sheet.row_values(i)\n",
    "        s,w,t = row[0], row[1], row[2]\n",
    "        if (curr_s == ''):\n",
    "            curr_s = s \n",
    "            if padded == True:\n",
    "                word_tags.append((pad_start_symbol,pad_start_symbol))\n",
    "                \n",
    "        elif s != '':\n",
    "            curr_s = s\n",
    "            if padded == True:                \n",
    "                word_tags.append((pad_end_symbol,pad_end_symbol))\n",
    "                word_tags.append((pad_start_symbol,pad_start_symbol))\n",
    "        word_tags.append((str(w),t))      \n",
    "        \n",
    "    if padded == True:\n",
    "        word_tags.append((pad_end_symbol,pad_end_symbol))        \n",
    "    return word_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTag = '_START_'\n",
    "endTag = '_END_'\n",
    "word_tags = sents_tags_tokenize(train_sheet, padded=True, pad_start_symbol=startTag, pad_end_symbol=endTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of word_tags: 95151\n",
      "--------------------------\n",
      "sample word_tags :  [('_START_', '_START_'), ('Palestinian', 'JJ'), ('Authority', 'NNP'), ('President', 'NNP'), ('Mahmoud', 'NNP'), ('Abbas', 'NNP'), ('is', 'VBZ'), ('expected', 'VBN'), ('to', 'TO'), ('ask', 'VB'), ('Hamas', 'NNP'), ('prime', 'JJ'), ('minister-designate', 'JJ'), ('Ismail', 'NNP'), ('Haniyeh', 'NNP'), ('to', 'TO'), ('form', 'VB'), ('a', 'DT'), ('new', 'JJ'), ('government', 'NN'), ('Tuesday', 'NNP'), ('.', '.'), ('_END_', '_END_'), ('_START_', '_START_'), ('Hamas', 'NNP'), ('wants', 'VBZ'), ('to', 'TO'), ('build', 'VB'), ('a', 'DT'), ('coalition', 'NN'), ('government', 'NN'), (',', ','), ('but', 'CC'), ('Mr.', 'NNP'), ('Abbas', 'NNP'), (\"'s\", 'POS'), ('Fatah', 'NNP'), ('party', 'NN'), ('and', 'CC'), ('the', 'DT'), ('militant', 'JJ'), ('Islamic', 'NNP'), ('Jihad', 'NNP'), ('group', 'NN'), ('have', 'VBP'), ('declined', 'VBN'), ('to', 'TO'), ('join', 'VB'), ('.', '.'), ('_END_', '_END_')]\n"
     ]
    }
   ],
   "source": [
    "print(\"len of word_tags:\",len(word_tags))\n",
    "print(\"--------------------------\")\n",
    "print(\"sample word_tags : \", word_tags[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "در بخش بعد برای بدست آوردن ماتریسهای Transition و Emission ابتدا تعداد دفعات تکرار هر pos و تعداد دفعات تکرار ترکیبهای دوتایی posها و همچنین تعداد دفعات تکرار هر زوج word,pos در دادگان آموزشی بدست آمده و سپس ماتریسهای trans_prob_matrix و emission_prob_matrix محاسبه شد. با توجه به اینکه تعداد احتمالات با مقدار صفر در این ماتریسها زیاد بوده لذا از روش add-one برای smoothing استفاده شد. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "words : مجموعه کلمات یونیک که سایز vocab خواهد بود\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "tagCountDict : .تعداد دفعات تکرار هر تگ را نگه مي‌دارد. هر تگی که به عنوان تگ قبلی استفاده شده است\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "bigramTagsCountDict : تعداد دفعات تکرار هر دو تگ پشت سر هم را نگه مي‌دارد\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "wordTagCountDict : تعداد دفعاتی که به یک کلمه یک tag خاص نسبت داده شده را نگه مي‌دارد\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {startTag,endTag} # set\n",
    "tagCountDict = {}         # dictionary\n",
    "bigramTagsCountDict = {}\n",
    "wordTagCountDict = {}\n",
    "\n",
    "prevTag = startTag\n",
    "for word,tag in word_tags:\n",
    "    words.add(word)\n",
    "    if prevTag not in tagCountDict:\n",
    "        tagCountDict[prevTag] = 1\n",
    "    else:\n",
    "        tagCountDict[prevTag] = tagCountDict[prevTag] + 1\n",
    "        \n",
    "    if (prevTag,tag) not in bigramTagsCountDict:\n",
    "        bigramTagsCountDict[(prevTag,tag)] = 1\n",
    "    else:\n",
    "        bigramTagsCountDict[(prevTag,tag)] = bigramTagsCountDict[(prevTag,tag)] + 1\n",
    "    prevTag = tag\n",
    "    \n",
    "    if (word,tag) not in wordTagCountDict:\n",
    "        wordTagCountDict[(word,tag)] = 1\n",
    "    else:\n",
    "        wordTagCountDict[(word,tag)] = wordTagCountDict[(word,tag)] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags_size: 42\n",
      "vocab_size: 10318\n",
      "\n",
      "len of tagCountDict: 42\n",
      "sample tagCountDict['NN']: 12158\n",
      "\n",
      "len of bigramTagsCountDict: 825\n",
      "sample bigramTagsCountDict[('VB', 'DT')]: 476\n",
      "\n",
      "len of wordTagCountDict: 11487\n",
      "sample wordTagCountDict[('hope', 'NN')]: 9\n"
     ]
    }
   ],
   "source": [
    "tags_size = len(tagCountDict)\n",
    "vocab_size = len(words)\n",
    "\n",
    "print(\"tags_size:\", tags_size)\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "\n",
    "print(\"\\nlen of tagCountDict:\", len(tagCountDict))\n",
    "print(\"sample tagCountDict['NN']:\", tagCountDict['NN'])\n",
    "\n",
    "print(\"\\nlen of bigramTagsCountDict:\", len(bigramTagsCountDict))\n",
    "print(\"sample bigramTagsCountDict[('VB', 'DT')]:\", bigramTagsCountDict[('VB', 'DT')])\n",
    "\n",
    "print(\"\\nlen of wordTagCountDict:\", len(wordTagCountDict))\n",
    "print(\"sample wordTagCountDict[('hope', 'NN')]:\", wordTagCountDict[('hope', 'NN')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "به هر کلمه و هر تگ، يک ایندکس عددی نسبت داده می شود تا در ماتریس براساس آن ایندکس، ارجاع شوند\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(tagCountDict.keys())\n",
    "tag2index = {t:i for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(words)\n",
    "word2index = {w:i for i, w in enumerate(words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "ماتریس Transition که ابعاد آن به اندازه tag_size * tag_size می باشد، ساخته می شود\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "هر درایه این ماتریس از تقسیم تعداد دفعات تکرار یک ترکیب دوتایی تگ به دفعات تکرار تگ اولی بدست می آید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Smoothing\n",
    "# Transition probabilities\n",
    "trans_prob_matrix = np.zeros([tags_size,tags_size])\n",
    "for pre_tag,tag in bigramTagsCountDict:\n",
    "    i = tag2index[pre_tag]\n",
    "    j = tag2index[tag]\n",
    "    trans_prob_matrix[i][j] = bigramTagsCountDict[(pre_tag,tag)]/ tagCountDict[pre_tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    " با توجه به اینکه تعداد احتمالات با مقدار صفر در این ماتریس زیاد بوده لذا از روش add-one برای smoothing استفاده شد. فرض شد که یک ترکیب دوتایی برای هر دو تگ انتخابی اضافه شده است (tag_size*tag_size زوج مرتب به دارگان آموزشی اضافه شده)\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Smoothing add-one\n",
    "# Transition probabilities\n",
    "trans_prob_matrix = np.zeros([tags_size,tags_size])\n",
    "for t_i in tags:\n",
    "    for t_j in tags:\n",
    "        i = tag2index[t_i]\n",
    "        j = tag2index[t_j]        \n",
    "        if (t_i,t_j) in bigramTagsCountDict:\n",
    "            trans_prob_matrix[i][j] = (bigramTagsCountDict[(t_i,t_j)]+1) / (tagCountDict[t_i]+tags_size)\n",
    "        else:\n",
    "            trans_prob_matrix[i][j] = 1/(tagCountDict[t_i]+tags_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "برای کنترل صحت مقداردهی ماتریس Transition، به ازای هر تگ، تمام احتمالات تگهای بعدی آن با هم جمع شد که باید برابر 1 شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum trans prob of tag  _START_ = 0.999999999999999\n",
      "sum trans prob of tag  JJ = 1.0\n",
      "sum trans prob of tag  NNP = 1.0000000000000004\n",
      "sum trans prob of tag  VBZ = 0.9999999999999996\n",
      "sum trans prob of tag  VBN = 0.9999999999999996\n",
      "sum trans prob of tag  TO = 1.0000000000000007\n",
      "sum trans prob of tag  VB = 1.0\n",
      "sum trans prob of tag  DT = 1.0000000000000004\n",
      "sum trans prob of tag  NN = 0.9999999999999997\n",
      "sum trans prob of tag  . = 0.9999999999999984\n",
      "sum trans prob of tag  _END_ = 0.9999999999999988\n",
      "sum trans prob of tag  , = 1.0\n",
      "sum trans prob of tag  CC = 0.9999999999999999\n",
      "sum trans prob of tag  POS = 0.9999999999999993\n",
      "sum trans prob of tag  VBP = 0.9999999999999997\n",
      "sum trans prob of tag  VBG = 1.0000000000000002\n",
      "sum trans prob of tag  IN = 1.0000000000000007\n",
      "sum trans prob of tag  NNS = 1.0000000000000002\n",
      "sum trans prob of tag  PRP = 1.0000000000000007\n",
      "sum trans prob of tag  RP = 1.0\n",
      "sum trans prob of tag  VBD = 1.0000000000000002\n",
      "sum trans prob of tag  MD = 1.0000000000000007\n",
      "sum trans prob of tag  JJR = 0.9999999999999997\n",
      "sum trans prob of tag  RB = 0.9999999999999994\n",
      "sum trans prob of tag  EX = 0.9999999999999996\n",
      "sum trans prob of tag  $ = 0.9999999999999998\n",
      "sum trans prob of tag  CD = 0.9999999999999996\n",
      "sum trans prob of tag  RBR = 1.0000000000000002\n",
      "sum trans prob of tag  PRP$ = 0.9999999999999998\n",
      "sum trans prob of tag  JJS = 0.9999999999999999\n",
      "sum trans prob of tag  WP = 1.0\n",
      "sum trans prob of tag  WRB = 0.9999999999999998\n",
      "sum trans prob of tag  WDT = 0.9999999999999998\n",
      "sum trans prob of tag  PDT = 1.0000000000000002\n",
      "sum trans prob of tag  `` = 0.9999999999999997\n",
      "sum trans prob of tag  NNPS = 1.0000000000000002\n",
      "sum trans prob of tag  RBS = 0.9999999999999992\n",
      "sum trans prob of tag  WP$ = 1.0000000000000009\n",
      "sum trans prob of tag  LRB = 0.9999999999999989\n",
      "sum trans prob of tag  RRB = 0.9999999999999992\n",
      "sum trans prob of tag  : = 0.9999999999999996\n",
      "sum trans prob of tag  ; = 0.9999999999999993\n"
     ]
    }
   ],
   "source": [
    "# Test of correctness of Transition Matrix\n",
    "for i in range(len(tags)):\n",
    "    sum = 0\n",
    "    for j in range(len(tags)):\n",
    "        sum = sum + trans_prob_matrix[i][j] \n",
    "    print (\"sum trans prob of tag \",tags[i],\"=\",sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "ماتریس Emission که ابعاد آن به اندازه vocab_size * tag_size می باشد، ساخته می شود\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "هر درایه این ماتریس از تقسیم تعداد دفعات تخصیص یک تگ به یک کلمه به دفعات تکرار آن تگ بدست می آید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Smoothing\n",
    "# Emission probabilities\n",
    "emission_prob_matrix = np.zeros([vocab_size,tags_size])\n",
    "for word,tag in wordTagCountDict:\n",
    "    i = word2index[word]\n",
    "    j = tag2index[tag]\n",
    "    emission_prob_matrix[i][j] = wordTagCountDict[(word,tag)]/tagCountDict[tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    " با توجه به اینکه تعداد احتمالات با مقدار صفر در این ماتریس زیاد بوده لذا از روش add-one برای smoothing استفاده شد. فرض شد که یک ترکیب دوتایی برای هر کلمه و تگ   اضافه شده است (word_size*tag_size زوج مرتب به دارگان آموزشی اضافه شده)\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Smoothing add-one\n",
    "# Emission probabilities\n",
    "emission_prob_matrix = np.zeros([vocab_size,tags_size])\n",
    "for w_i in words:\n",
    "    for t_j in tags:\n",
    "        i = word2index[w_i]\n",
    "        j = tag2index[t_j]        \n",
    "        if (w_i,t_j) in wordTagCountDict:\n",
    "            emission_prob_matrix[i][j] = (wordTagCountDict[(w_i,t_j)]+1)/ (tagCountDict[t_j]+vocab_size)\n",
    "        else:\n",
    "            emission_prob_matrix[i][j] = 1/(tagCountDict[t_j]+vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "برای کنترل صحت مقداردهی ماتریسEmission، به ازای هر تگ، تمام احتمالات تخصیص آن به کلمات مختلف با هم جمع شد که باید برابر 1 شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum emission prob of tag  _START_ = 0.9999301627207909\n",
      "sum emission prob of tag  JJ = 1.0000000000001843\n",
      "sum emission prob of tag  NNP = 0.9999999999999767\n",
      "sum emission prob of tag  VBZ = 1.0000000000001812\n",
      "sum emission prob of tag  VBN = 1.0000000000001892\n",
      "sum emission prob of tag  TO = 1.0000000000000877\n",
      "sum emission prob of tag  VB = 1.0000000000002072\n",
      "sum emission prob of tag  DT = 1.0000000000001723\n",
      "sum emission prob of tag  NN = 1.0000000000001719\n",
      "sum emission prob of tag  . = 1.0000000000002072\n",
      "sum emission prob of tag  _END_ = 1.0000698470349012\n",
      "sum emission prob of tag  , = 0.9999999999998457\n",
      "sum emission prob of tag  CC = 1.0000000000000442\n",
      "sum emission prob of tag  POS = 1.000000000000121\n",
      "sum emission prob of tag  VBP = 0.9999999999999067\n",
      "sum emission prob of tag  VBG = 1.0000000000001656\n",
      "sum emission prob of tag  IN = 1.0000000000002238\n",
      "sum emission prob of tag  NNS = 0.9999999999999005\n",
      "sum emission prob of tag  PRP = 1.0000000000001208\n",
      "sum emission prob of tag  RP = 0.9999999999998023\n",
      "sum emission prob of tag  VBD = 0.9999999999998734\n",
      "sum emission prob of tag  MD = 0.9999999999998472\n",
      "sum emission prob of tag  JJR = 1.0000000000000122\n",
      "sum emission prob of tag  RB = 0.9999999999998807\n",
      "sum emission prob of tag  EX = 0.9999999999997736\n",
      "sum emission prob of tag  $ = 1.0000000000000226\n",
      "sum emission prob of tag  CD = 0.9999999999998413\n",
      "sum emission prob of tag  RBR = 1.0000000000002518\n",
      "sum emission prob of tag  PRP$ = 1.000000000000085\n",
      "sum emission prob of tag  JJS = 0.999999999999882\n",
      "sum emission prob of tag  WP = 0.9999999999998134\n",
      "sum emission prob of tag  WRB = 1.000000000000162\n",
      "sum emission prob of tag  WDT = 1.0000000000001412\n",
      "sum emission prob of tag  PDT = 0.999999999999798\n",
      "sum emission prob of tag  `` = 1.0000000000000857\n",
      "sum emission prob of tag  NNPS = 0.999999999999808\n",
      "sum emission prob of tag  RBS = 0.9999999999999434\n",
      "sum emission prob of tag  WP$ = 0.9999999999999835\n",
      "sum emission prob of tag  LRB = 0.9999999999998141\n",
      "sum emission prob of tag  RRB = 0.999999999999817\n",
      "sum emission prob of tag  : = 0.9999999999998851\n",
      "sum emission prob of tag  ; = 0.9999999999999335\n"
     ]
    }
   ],
   "source": [
    "# Test of correctness of Emission Matrix\n",
    "for i in range(len(tags)):\n",
    "    sum = 0\n",
    "    for j in range(len(words)):\n",
    "        sum = sum + emission_prob_matrix[j][i] \n",
    "    print (\"sum emission prob of tag \",tags[i],\"=\",sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "الگوریتم viterbi پیاده سازی شده است. ورودی آن یک sequence از کلمات یک جمله است و خروجی آن sequence ای از تگ های تخصیص داده شده به کلمات جمله ورودی می باشد\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "در این الگوریتم با استفاده از روش dynamic programming ماتریس viterbi و backtrack به ابعاد تعداد تگها در تعداد کلمات ورودی ساخته و پر می شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(Seq):    \n",
    "    seq_len = len(Seq)\n",
    "    h, w = tags_size, seq_len\n",
    "    viterbi_matrix = [[0 for x in range(w)] for y in range(h)]\n",
    "    backtrack_matrix = [[0 for x in range(w)] for y in range(h)]\n",
    "    #initialize first column of viterbi_matrix\n",
    "    for s in tags:\n",
    "        state_ind = tag2index[s]\n",
    "        curr_word = Seq[0]\n",
    "        \n",
    "        if curr_word in words:\n",
    "            curr_word_ind = word2index[curr_word]\n",
    "            multPE = emission_prob_matrix[curr_word_ind][state_ind]\n",
    "        else:\n",
    "            multPE = 1\n",
    "        \n",
    "        multPT = trans_prob_matrix[tag2index[startTag]][state_ind]\n",
    "        \n",
    "        viterbi_matrix[state_ind][0] = multPT * multPE\n",
    "        backtrack_matrix[state_ind][0] = 0\n",
    "        \n",
    "    score = 0       \n",
    "    for t in range(1, seq_len):\n",
    "        for s_to in tags: \n",
    "            s_to_ind = tag2index[s_to]\n",
    "            if Seq[t] in words:\n",
    "                curr_word_ind = word2index[Seq[t]]\n",
    "                multPE = emission_prob_matrix[curr_word_ind][s_to_ind]\n",
    "            else:\n",
    "                multPE = 1\n",
    "            # find best score from previous step\n",
    "            f = 0\n",
    "            for s_from in tags: \n",
    "                s_from_ind = tag2index[s_from]\n",
    "                multPT = trans_prob_matrix[s_from_ind][s_to_ind]\n",
    "                score = viterbi_matrix[s_from_ind][t-1] * multPT * multPE\n",
    "                \n",
    "                if f == 0:\n",
    "                    viterbi_matrix[s_to_ind][t] = score\n",
    "                    backtrack_matrix[s_to_ind][t] = s_from\n",
    "                    f = 1\n",
    "                if score > viterbi_matrix[s_to_ind][t]:\n",
    "                    viterbi_matrix[s_to_ind][t] = score\n",
    "                    backtrack_matrix[s_to_ind][t] = s_from\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "    result_pairs = []\n",
    "    best_score_ind = 0\n",
    "    for i in range(len(tags)):\n",
    "        if viterbi_matrix[i][seq_len-1] > viterbi_matrix[best_score_ind][seq_len-1]:\n",
    "            best_score_ind = i\n",
    "    \n",
    "    result_pairs.append((Seq[seq_len-1],tags[best_score_ind]))    \n",
    "    for t in range(seq_len-1, 0, -1):\n",
    "        #print(t)\n",
    "        best_tag = backtrack_matrix[best_score_ind][t]\n",
    "        if best_tag != 0:\n",
    "            best_score_ind = tag2index[best_tag]\n",
    "        else:\n",
    "            best_score_ind=0\n",
    "        result_pairs.insert(0,(Seq[t-1],best_tag))\n",
    "    return result_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U.S.-led', 'JJ'),\n",
       " ('coalition', 'NN'),\n",
       " ('forces', 'NNS'),\n",
       " ('prevented', 'IN'),\n",
       " ('another', 'DT'),\n",
       " ('attack', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('discovering', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('bomb', 'NN'),\n",
       " ('near', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('eastern', 'JJ'),\n",
       " ('city', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Jalalabad', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi(['U.S.-led', 'coalition', 'forces', 'prevented', 'another', 'attack', 'by', 'discovering', \n",
    "         'a', 'bomb', 'near', 'the', 'eastern', 'city', 'of', 'Jalalabad', '.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "حال دادگان تست به کمک متد new_sents_tags_tokenize  به صورت دو لیست از sequence ها تبدیل شد. یک لیست برای sequence های جملات و یک لیست حاوی sequence های تگهای صحیح متناظر.  سپس sequence کلمات هر جمله به الگوریتم viterbi داده شد و تگهای predict شده بدست آمد و سپس به کمک کتابخانه sklearn.metrics مقادیر accuracy، recall و precision محاسبه گردید\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wb = xlrd.open_workbook('data/viterbi_test.xlsx') \n",
    "test_sheet = test_wb.sheet_by_index(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_sents_tags_tokenize(data_sheet, padded, pad_start_symbol, pad_end_symbol):\n",
    "    sents = []\n",
    "    sents_tags = []\n",
    "    \n",
    "    curr_s = ''\n",
    "    curr_sent=[]\n",
    "    curr_sent_tags=[]\n",
    "    for i in range(data_sheet.nrows):\n",
    "        row = data_sheet.row_values(i)\n",
    "        s,w,t = row[0], row[1], row[2]\n",
    "        if (curr_s == ''):\n",
    "            curr_s = s \n",
    "            if padded == True:\n",
    "                curr_sent.append(pad_start_symbol)\n",
    "                curr_sent_tags.append(pad_start_symbol)\n",
    "                \n",
    "        elif s != '':\n",
    "            curr_s = s\n",
    "            if padded == True:\n",
    "                curr_sent.append(pad_end_symbol)\n",
    "                curr_sent_tags.append(pad_end_symbol)\n",
    "            sents.append(curr_sent)\n",
    "            sents_tags.append(curr_sent_tags)\n",
    "            \n",
    "            curr_sent=[]\n",
    "            curr_sent_tags=[]\n",
    "            if padded == True:\n",
    "                curr_sent.append(pad_start_symbol)\n",
    "                curr_sent_tags.append(pad_start_symbol)\n",
    "\n",
    "        curr_sent.append(str(w))\n",
    "        curr_sent_tags.append(t)\n",
    "        \n",
    "    if  len(curr_sent) > 0:\n",
    "        if padded == True:\n",
    "            curr_sent.append(pad_end_symbol)\n",
    "            curr_sent_tags.append(pad_end_symbol)\n",
    "        \n",
    "        sents.append(curr_sent)\n",
    "        sents_tags.append(curr_sent_tags)\n",
    "        \n",
    "    return sents, sents_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents, sents_tags = new_sents_tags_tokenize(test_sheet, padded=True, pad_start_symbol=startTag, pad_end_symbol=endTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of sents: 1001 \n",
      "len of sents_tags: 1001\n",
      "--------------------------\n",
      "sample sentence :  ['_START_', 'The', 'press', 'freedom', 'group', 'Reporters', 'Without', 'Borders', 'has', 'condemned', 'the', 'decision', 'by', 'Venezuelan', 'President', 'Hugh', 'Chavez', 'to', 'shut', 'down', 'one', 'of', 'the', 'country', \"'s\", 'oldest', 'television', 'stations', 'The', 'group', 'issued', 'a', 'statement', 'Friday', 'calling', 'the', 'move', '\"', 'a', 'serious', 'attack', 'against', 'editorial', 'pluralism', '.', '\"', '_END_']\n",
      "sample sentence tags :  ['_START_', 'DT', 'NN', 'NN', 'NN', 'VBZ', 'IN', 'NNS', 'VBZ', 'VBN', 'DT', 'NN', 'IN', 'JJ', 'NNP', 'NNP', 'NNP', 'TO', 'VB', 'RP', 'CD', 'IN', 'DT', 'NN', 'POS', 'JJS', 'NN', 'NNS', 'DT', 'NN', 'VBD', 'DT', 'NN', 'NNP', 'VBG', 'DT', 'NN', '``', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', '.', '``', '_END_']\n"
     ]
    }
   ],
   "source": [
    "print(\"len of sents:\",len(sents),\"\\nlen of sents_tags:\",len(sents_tags))\n",
    "print(\"--------------------------\")\n",
    "print(\"sample sentence : \", sents[1])\n",
    "print(\"sample sentence tags : \", sents_tags[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of different tags in test data: 42\n",
      "tags of test data that aren't in train data {'UH'}\n",
      "tags of train data that aren't in test data {'PDT'}\n"
     ]
    }
   ],
   "source": [
    "test_tags = []\n",
    "for s in sents_tags:\n",
    "    for t in s:\n",
    "        test_tags.append(t)\n",
    "test_tags = set(test_tags)        \n",
    "print(\"size of different tags in test data:\", len(test_tags))\n",
    "print(\"tags of test data that aren't in train data\", test_tags - set(tags))\n",
    "print(\"tags of train data that aren't in test data\", set(tags) - test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "با توجه به اینکه در داده های تست تگ UH وجود دارد که در داده های تست نبوده لذا باید هنگام ارزیابی آنرا در نظر گرفت و شرطهای لازم را برای جلوگیری از خطا اعمال کرد\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "y_pred = []\n",
    "#sents = sents[:200]\n",
    "for i in range(len(sents)):\n",
    "    assigned_tags = viterbi(sents[i])\n",
    "    \n",
    "    for j in range(len(sents[i])):\n",
    "        if sents_tags[i][j] in tags:\n",
    "            y_test.append(tag2index[sents_tags[i][j]])\n",
    "        else:\n",
    "            y_test.append(50) # for tag UH that not exists in tags of train data\n",
    "        y_pred.append(tag2index[assigned_tags[j][1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision :  0.880666584168626\n",
      "recall :  0.880666584168626\n",
      "f1_score :  0.880666584168626\n",
      "accuracy_score :  0.880666584168626\n"
     ]
    }
   ],
   "source": [
    "print(\"precision : \", precision_score(y_test, y_pred, average=\"micro\"))\n",
    "print(\"recall : \", recall_score(y_test, y_pred, average=\"micro\")) \n",
    "print(\"f1_score : \", f1_score(y_test, y_pred, average=\"micro\"))\n",
    "print(\"accuracy_score : \", accuracy_score(y_test, y_pred))\n",
    "#print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<font style=\"font-family:verdana\" size=\"3\" color=\"Black\">    \n",
    "نتایج بدست آمده در یک فایل csv نوشته شد تا بعدا به ستون F فایل viterbi_test.xlsx اضافه شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22241"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_tag_ind = tag2index[startTag]\n",
    "end_tag_ind = tag2index[endTag]\n",
    "result_tags = []\n",
    "for i in range(len(y_test)):    \n",
    "    if y_test[i] != start_tag_ind and y_test[i] != end_tag_ind:\n",
    "        result_tags.append(tags[y_pred[i]])\n",
    "len(result_tags)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hmm_file = open('test_hmm_tags.csv', \"w\")    \n",
    "np.savetxt(test_hmm_file, result_tags, fmt=\"%s\", delimiter=\",\")\n",
    "test_hmm_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
